# Set Up a Separate VM as a Load Balancer

Here’s a detailed step-by-step guide to set up a separate VM as a load balancer for your Kubernetes worker nodes using NGINX and an NGINX Ingress Controller on your cluster.

## Overview

1. **Load Balancer VM**: This will handle HTTP/HTTPS requests and distribute them to your Kubernetes worker nodes.
1. **Kubernetes Cluster**: You have worker nodes running services in the cluster.
1. **NGINX Ingress Controller**: This will handle traffic routing based on your Kubernetes Ingress resources.

## Prerequisites

1. You have a Kubernetes cluster with at least 2 worker nodes, say:

   - Worker node 1: `192.168.1.51`
   - Worker node 2: `192.168.1.52`

1. Your NGINX load balancer VM has the private IP `192.168.1.40`.

1. Your router's public IP is `81.10.0.7`.

1. Domain name is `hadywafa.com`, and it should be set up to point to your public IP (`81.10.0.7`).

1. Your router should be configured to forward incoming HTTP and HTTPS traffic from your public IP (81.10.0.7) to the NGINX load balancer VM (192.168.1.40).

   - HTTP Forwarding:
     - `81.10.0.7:80` → `192.168.1.40:80` (for HTTP)
   - HTTPS Forwarding:
     - `81.10.0.7:443` → `192.168.1.40:443` (for HTTPS)

## 1. **Set Up the Load Balancer VM**

1. **Provision a New VM**:
   - Set up a VM with at least **1 GB of RAM** (2 GB recommended) and install a Linux distribution such as Ubuntu.
   - Ensure the VM is in the same network as your Kubernetes cluster, and can communicate with your worker nodes (e.g., `192.168.1.51`, `192.168.1.52`).

## 2. **Install NGINX on the Load Balancer VM**

1. **Update the package list** and install NGINX:

   ```bash
   sudo apt update
   sudo apt install nginx
   ```

2. **Enable and start NGINX**:

   ```bash
   sudo systemctl enable nginx
   sudo systemctl start nginx
   ```

3. **Verify NGINX is running**:

   ```bash
   sudo systemctl status nginx
   ```

   You should see that NGINX is active and running.

## 3. **Configure NGINX as a Load Balancer**

We will configure NGINX to distribute HTTP and HTTPS traffic across your Kubernetes worker nodes.

1. **Create a new NGINX configuration file**:

   ```bash
   sudo nano /etc/nginx/sites-available/loadbalancer
   ```

2. **Add the following configuration** to load balance traffic between the worker nodes (`192.168.1.51` and `192.168.1.52`) for both HTTP and HTTPS:

   ```nginx
   # Upstream configuration for HTTP traffic
   upstream k8s_http {
       server 192.168.1.51:32680;  # Worker Node 1 HTTP NodePort
       server 192.168.1.52:32680;  # Worker Node 2 HTTP NodePort
       keepalive 32;
   }

   # Upstream configuration for HTTPS traffic
   upstream k8s_https {
       server 192.168.1.51:31064;  # Worker Node 1 HTTPS NodePort
       server 192.168.1.52:31064;  # Worker Node 2 HTTPS NodePort
       keepalive 32;
   }

   # HTTP Server Block
   server {
       listen 80;
       server_name *.hadywafa.com;

       location / {
           proxy_pass http://k8s_http;
           proxy_set_header Host $host;
           proxy_set_header X-Real-IP $remote_addr;
           proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
           proxy_set_header X-Forwarded-Proto $scheme;
       }

       access_log /var/log/nginx/access.log upstreamlog;
       error_log /var/log/nginx/error.log;
   }

   # HTTPS Server Block
   server {
       listen 443 ssl;
       server_name *.hadywafa.com;

       ssl_certificate /etc/letsencrypt/live/hadywafa.com/fullchain.pem;
       ssl_certificate_key /etc/letsencrypt/live/hadywafa.com/privkey.pem;

       ssl_protocols TLSv1.2 TLSv1.3;
       ssl_ciphers HIGH:!aNULL:!MD5;

       location / {
           proxy_pass https://k8s_https;
           proxy_set_header Host $host;
           proxy_set_header X-Real-IP $remote_addr;
           proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
           proxy_set_header X-Forwarded-Proto $scheme;
       }

       access_log /var/log/nginx/access.log upstreamlog;
       error_log /var/log/nginx/error.log;
   }
   ```

3. **Link the configuration** to enable it:

   ```bash
   sudo ln -s /etc/nginx/sites-available/loadbalancer /etc/nginx/sites-enabled/
   ```

4. **Test the NGINX configuration** for syntax errors:

   ```bash
   sudo nginx -t
   ```

5. **Reload NGINX** to apply the changes:

   ```bash
   sudo systemctl reload nginx
   ```

## 4. **Set Up SSL with Let’s Encrypt (Optional for HTTPS)**

If you want to support HTTPS traffic, follow these steps to set up SSL using Let’s Encrypt.

1. **Install Certbot**:

   ```bash
   sudo apt install certbot python3-certbot-nginx
   ```

2. **Obtain an SSL certificate**:

   ```bash
   sudo certbot --nginx -d hadywafa.com
   ```

   This command will automatically configure your NGINX server for HTTPS.

3. **Verify SSL configuration**:

   ```bash
   sudo nginx -t
   sudo systemctl reload nginx
   ```

4. **Set up automatic certificate renewal**:

   ```bash
   sudo certbot renew --dry-run
   ```

## 5. **Configure DNS to Point to the Load Balancer**

Make sure your domain (`hadywafa.com`) points to the **public IP** of your load balancer.

1. **Set up an A record** for `hadywafa.com` in your DNS provider to point to the public IP of your load balancer VM.

2. **Verify DNS resolution**:

   ```bash
   nslookup hadywafa.com
   ```

   You should see the public IP of your load balancer.

## 6. **Deploy the Sample Application (myapp)**

To test the load balancer, you’ll deploy a sample application (`myapp`) in your Kubernetes cluster.

1. **Create the `myapp` deployment**:

   Save the following YAML as `myapp-deployment.yaml`:

   ```yaml
   apiVersion: apps/v1
   kind: Deployment
   metadata:
     name: myapp-deployment
   spec:
     replicas: 3
     selector:
       matchLabels:
         app: myapp
     template:
       metadata:
         labels:
           app: myapp
       spec:
         containers:
           - name: myapp
             image: nginx
             ports:
               - containerPort: 80
   ```

   Apply the deployment:

   ```bash
   kubectl apply -f myapp-deployment.yaml
   ```

2. **Create the `myapp` service**:

   Save the following YAML as `myapp-service.yaml`:

   ```yaml
   apiVersion: v1
   kind: Service
   metadata:
     name: myapp-service
   spec:
     selector:
       app: myapp
     ports:
       - protocol: TCP
         port: 80
         targetPort: 80
     type: ClusterIP
   ```

   Apply the service:

   ```bash
   kubectl apply -f myapp-service.yaml
   ```

3. **Verify the deployment**:

   ```bash
   kubectl get pods
   kubectl get svc
   ```

## 7. **Test the Load Balancer**

Once the load balancer is configured and the `myapp` service is running, you can test if the load balancer is working properly.

1. **Access the application through the load balancer**:

   - **HTTP**: Visit `http://hadywafa.com`.
   - **HTTPS**: Visit `https://hadywafa.com`.

2. **Use Curl to Test Internally**:

   ```bash
   curl http://hadywafa.com
   curl https://hadywafa.com
   ```

3. **Check NGINX Logs**:

   ```bash
   sudo tail -f /var/log/nginx/access.log
   ```

   This will show how requests are being forwarded to the worker nodes.

## 8. **Monitor and Optimize NGINX**

1. **Monitor resource usage**:

   Use `top` or `htop` to monitor CPU and memory usage on the load balancer.

   ```bash
   top
   ```

2. **Optimize NGINX performance** by adjusting worker processes and keepalive settings:

   ```nginx
   worker_processes auto;
   keepalive_timeout 65;
   keepalive_requests 100;
   ```

## Summary

1. **Set up the VM and install NGINX**.
2. **Configure NGINX as a load balancer** for your Kubernetes worker nodes.
3. **Optionally set up SSL** with Let’s Encrypt for HTTPS traffic.
4. **Configure DNS** to point your domain to the load balancer’s public IP.
5. **Deploy and test the sample `myapp`** to ensure load balancing works correctly.
6. **Monitor and optimize** the load balancer’s performance.

By following these steps, you'll have a fully functioning load balancer that distributes traffic across your Kubernetes worker nodes. Let me know if you need further assistance!

## Benefits of Using a Separate VM as a Load Balancer

- **Better Distribution**: The load balancer distributes traffic between your worker nodes, balancing the load and ensuring availability.
- **Offload Traffic**: You reduce the pressure on the master node, which no longer has to handle all incoming traffic.
- **Scalability**: If you add more worker nodes in the future, you can easily configure the load balancer to distribute traffic to additional nodes.
- **Flexibility**: You have full control over how traffic is distributed (e.g., round-robin, least connections, etc.).
