# k8s Questions

## What is the difference between Deployment Controller and ReplicaSet Controller

### Deployment Controller

- **Role**: Overseer and Decision-Maker
- **Responsibilities**:
  - Watches the Deployment resource for any desired state changes (e.g., scaling up/down, updating the container image).
  - Makes decisions on what actions need to be taken to achieve the desired state.
  - Creates, updates, or deletes ReplicaSets as needed to match the Deployment's specifications.
  - Manages rolling updates and rollbacks by creating new ReplicaSets or updating existing ones.

### ReplicaSet Controller

- **Role**: Executor
- **Responsibilities**:
  - Watches the ReplicaSet resource.
  - Ensures that the specified number of pod replicas are running at all times.
  - If a pod managed by the ReplicaSet fails or is deleted, the ReplicaSet Controller creates a new pod to replace it.

### Interaction Between Deployment Controller and ReplicaSet Controller

1. **Deployment Creation**:
   - You create a Deployment specifying the desired state (e.g., 3 replicas of a web server).
   - The Deployment Controller sees this new Deployment and creates a corresponding ReplicaSet to match the desired state.

2. **Scaling**:
   - If you update the Deployment to scale the number of replicas from 3 to 5, the Deployment Controller detects this change.
   - The Deployment Controller updates the ReplicaSet to specify 5 replicas.
   - The ReplicaSet Controller sees this updated ReplicaSet and creates 2 additional pods to meet the new desired state of 5 replicas.

3. **Rolling Update**:
   - If you update the Deployment with a new container image, the Deployment Controller handles the rolling update process.
   - The Deployment Controller creates a new ReplicaSet with the updated container image.
   - The Deployment Controller gradually scales up the new ReplicaSet and scales down the old one to perform the rolling update.
   - The ReplicaSet Controller ensures the correct number of pods are running during this process, creating new pods from the new ReplicaSet and deleting old pods from the old ReplicaSet.

4. **Pod Failure**:
   - If a pod managed by the ReplicaSet fails, the ReplicaSet Controller detects this failure.
   - The ReplicaSet Controller creates a new pod to replace the failed one to maintain the desired number of replicas.

### Visual Representation

```txt
[Deployment]
     |
     v
[Deployment Controller]
     |
     v
[ReplicaSet]
     |
     v
[ReplicaSet Controller]
     |
     v
[Pods]
```

In summary, the **Deployment Controller** acts as the overseer and decision-maker, making high-level decisions based on the Deployment specifications. It delegates the task of ensuring the correct number of pod replicas to the **ReplicaSet Controller**, which acts as the executor, ensuring that the actual state matches the desired state by creating or deleting pods as necessary.

## How do controllers detect changes in the state of the cluster?

- Each Kubernetes controller subscribes to specific events from the API server.
- These events are triggered when actions such as creating, updating, or deleting pods occur, or when the system handles node failures.
- When a user makes changes to the configuration, the API server stores the new settings in etcd and then triggers relevant events.
- Other controllers in the cluster subscribe to these events and execute their respective logic to ensure that the cluster maintains its desired state.

## How Node Failure Detected in Cluster?

### 1. Regular Heartbeats

- **Kubelet Heartbeats**: Each node in a Kubernetes cluster runs a Kubelet. The Kubelet is responsible for monitoring the node and the pods running on it. It sends regular heartbeats (node status updates) to the API server to indicate that it is healthy and operational. These heartbeats are sent at regular intervals (default is every 10 seconds).

### 2. Node Controller Monitoring

- **Node Controller**: The Node Controller, which runs as part of the kube-controller-manager on the control plane, is responsible for monitoring the status of each node in the cluster.
- **Heartbeat Timeout**: The Node Controller watches for the heartbeats sent by the Kubelet. If it does not receive a heartbeat from a node within a certain timeout period (default is 40 seconds), it marks the node as "NotReady".

### 3. Updating Node Status

- **Marking the Node**: When the Node Controller detects that a node has missed several heartbeats (based on the timeout), it updates the node's status in etcd via the API server to reflect that the node is "NotReady".
- **Pod Eviction**: The Node Controller then starts the process of evicting the pods running on the "NotReady" node. This process involves marking the pods for deletion, so they can be rescheduled to other healthy nodes.

### 4. Rescheduling Pods

- **Pod Deletion and Recreation**: Controllers like the ReplicaSet Controller or Deployment Controller detect that pods have been marked for deletion. They then ensure that new pods are created to maintain the desired state of the application.
- **Scheduler Role**: The Kubernetes Scheduler identifies healthy nodes with available resources to place the new pods, ensuring that the application remains available despite the node failure.

### Behind the Scenes

Hereâ€™s a simplified step-by-step summary of what happens when a node fails:

1. **Kubelet on the Node Stops Sending Heartbeats**: Due to the node failure, the Kubelet on that node stops sending heartbeats to the API server.

2. **Node Controller Detects the Missing Heartbeats**: The Node Controller running in the kube-controller-manager on the control plane notices that it hasn't received a heartbeat from the node within the expected time frame.

3. **Node Marked as "NotReady"**: The Node Controller updates the node's status in etcd via the API server to "NotReady".

4. **Pod Eviction Begins**: The Node Controller starts marking the pods on the "NotReady" node for deletion.

5. **Pod Deletion Detected by Other Controllers**: The ReplicaSet Controller, Deployment Controller, or other relevant controllers detect that pods have been marked for deletion and create new pods to replace them.

6. **Scheduler Reschedules Pods**: The Kubernetes Scheduler identifies suitable healthy nodes for the new pods and schedules them accordingly.

### Diagram: Node Failure Handling

1. **Kubelet** on the node sends regular heartbeats to the **API Server**.
2. **Node Controller** monitors these heartbeats.
3. Node failure -> Heartbeats stop.
4. **Node Controller** marks the node as "NotReady" in etcd.
5. **Controllers** (ReplicaSet, Deployment) detect pod deletions.
6. New pods are scheduled by the **Scheduler** on healthy nodes.

This process ensures that even in the event of a node failure, Kubernetes can quickly detect the issue and take corrective actions to maintain the desired state of the cluster.

Sure! Let's dive into how Kubernetes detects pod failures and handles them.

## How Pod/Container Failure Detected in Cluster?

### 1. Kubelet Monitoring

- **Kubelet Role**: The Kubelet, which runs on each node, is responsible for monitoring the health of the pods running on that node. It uses liveness probes and readiness probes to check the health of each container within a pod.

### 2. Probes

- **Liveness Probes**: These are used to determine if a container is still running. If a liveness probe fails, the Kubelet will kill the container and the container will be subject to its restart policy.
  - **Types of Liveness Probes**:
    - **HTTP Probe**: Kubelet sends an HTTP request to the container.
    - **TCP Probe**: Kubelet attempts to open a TCP connection to the container.
    - **Exec Probe**: Kubelet runs a specified command inside the container.

- **Readiness Probes**: These are used to determine if a container is ready to serve traffic. If a readiness probe fails, the endpoints controller removes the pod from the endpoints of all services that match the pod.

### 3. Container Restart Policy

- **Restart Policies**: Kubernetes allows you to specify a restart policy for containers within a pod:
  - **Always**: The container will be restarted regardless of the exit status.
  - **OnFailure**: The container will be restarted only if it exits with a non-zero status.
  - **Never**: The container will not be restarted.

### 4. Health Check Failure

- **Probe Failure**: If a liveness or readiness probe fails, the Kubelet takes appropriate action:
  - **Liveness Probe Failure**: The Kubelet kills the container. Depending on the restart policy, the container may be restarted.
  - **Readiness Probe Failure**: The Kubelet marks the pod as "NotReady", and it will not receive any traffic until the probe passes.

### 5. Pod Deletion and Recreation

- **Controller Monitoring**: Higher-level controllers (such as Deployment or ReplicaSet controllers) monitor the status of pods. If a pod fails or is deleted, these controllers ensure that a new pod is created to maintain the desired number of replicas.

### 6. Handling Node Failures

- **Node Controller**: In addition to Kubelet health checks, the Node Controller also plays a role in pod failure detection by monitoring node status. If a node becomes "NotReady", the Node Controller marks the pods on that node as failed and triggers rescheduling.

### Example Scenario: Pod Failure Detection and Recovery

1. **Kubelet on Node**: Continuously monitors the health of pods using liveness and readiness probes.
2. **Health Check Failure**:
   - Liveness probe fails for a container -> Kubelet kills the container.
   - Readiness probe fails for a container -> Kubelet marks the pod as "NotReady".
3. **Container Restart**:
   - Kubelet restarts the container based on the restart policy.
4. **Pod Failure Handling**:
   - If a pod is marked as failed and deleted, the ReplicaSet or Deployment controller detects the failure.
   - The controller creates a new pod to replace the failed one.
5. **Node Failure**:
   - Node Controller detects a node is "NotReady".
   - Marks all pods on the node as failed.
   - Higher-level controllers (ReplicaSet, Deployment) create new pods on healthy nodes.

### Summary

- **Kubelet** on each node monitors pod health using liveness and readiness probes.
- **Liveness Probes** check if a container is running, and **Readiness Probes** check if a container is ready to serve traffic.
- The **Kubelet** takes actions (e.g., restarting containers, marking pods as "NotReady") based on probe results.
- **Higher-level controllers** (e.g., ReplicaSet, Deployment) monitor and maintain the desired state of pods.
- **Node Controller** handles node-level failures and triggers rescheduling of pods.

This monitoring and recovery mechanism ensures that Kubernetes can quickly detect and recover from pod failures, maintaining the health and availability of applications running in the cluster.
