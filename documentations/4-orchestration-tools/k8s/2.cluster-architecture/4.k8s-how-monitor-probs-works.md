# Components Failures Detection

## Node Failure Detection in Kubernetes

### 1. Regular Heartbeats

- **Kubelet Heartbeats**: Each node in a Kubernetes cluster runs a Kubelet. The Kubelet is responsible for monitoring the node and the pods running on it. It sends regular heartbeats (node status updates) to the API server to indicate that it is healthy and operational. These heartbeats are sent at regular intervals (default is every 10 seconds).

### 2. Node Controller Monitoring

- **Node Controller**: The Node Controller, which runs as part of the kube-controller-manager on the control plane, is responsible for monitoring the status of each node in the cluster.
- **Heartbeat Timeout**: The Node Controller watches for the heartbeats sent by the Kubelet. If it does not receive a heartbeat from a node within a certain timeout period (default is 40 seconds), it marks the node as "NotReady".

### 3. Updating Node Status

- **Marking the Node**: When the Node Controller detects that a node has missed several heartbeats (based on the timeout), it updates the node's status in etcd via the API server to reflect that the node is "NotReady".
- **Pod Eviction**: The Node Controller then starts the process of evicting the pods running on the "NotReady" node. This process involves marking the pods for deletion, so they can be rescheduled to other healthy nodes.

### 4. Rescheduling Pods

- **Pod Deletion and Recreation**: Controllers like the ReplicaSet Controller or Deployment Controller detect that pods have been marked for deletion. They then ensure that new pods are created to maintain the desired state of the application.
- **Scheduler Role**: The Kubernetes Scheduler identifies healthy nodes with available resources to place the new pods, ensuring that the application remains available despite the node failure.

### Behind the Scenes

Hereâ€™s a simplified step-by-step summary of what happens when a node fails:

1. **Kubelet on the Node Stops Sending Heartbeats**: Due to the node failure, the Kubelet on that node stops sending heartbeats to the API server.

2. **Node Controller Detects the Missing Heartbeats**: The Node Controller running in the kube-controller-manager on the control plane notices that it hasn't received a heartbeat from the node within the expected time frame.

3. **Node Marked as "NotReady"**: The Node Controller updates the node's status in etcd via the API server to "NotReady".

4. **Pod Eviction Begins**: The Node Controller starts marking the pods on the "NotReady" node for deletion.

5. **Pod Deletion Detected by Other Controllers**: The ReplicaSet Controller, Deployment Controller, or other relevant controllers detect that pods have been marked for deletion and create new pods to replace them.

6. **Scheduler Reschedules Pods**: The Kubernetes Scheduler identifies suitable healthy nodes for the new pods and schedules them accordingly.

### Diagram: Node Failure Handling

1. **Kubelet** on the node sends regular heartbeats to the **API Server**.
2. **Node Controller** monitors these heartbeats.
3. Node failure -> Heartbeats stop.
4. **Node Controller** marks the node as "NotReady" in etcd.
5. **Controllers** (ReplicaSet, Deployment) detect pod deletions.
6. New pods are scheduled by the **Scheduler** on healthy nodes.

This process ensures that even in the event of a node failure, Kubernetes can quickly detect the issue and take corrective actions to maintain the desired state of the cluster.

Sure! Let's dive into how Kubernetes detects pod failures and handles them.

## Pod/Container Failure Detection in Kubernetes

### 1. Kubelet Monitoring

- **Kubelet Role**: The Kubelet, which runs on each node, is responsible for monitoring the health of the pods running on that node. It uses liveness probes and readiness probes to check the health of each container within a pod.

### 2. Probes

- **Liveness Probes**: These are used to determine if a container is still running. If a liveness probe fails, the Kubelet will kill the container and the container will be subject to its restart policy.
  - **Types of Liveness Probes**:
    - **HTTP Probe**: Kubelet sends an HTTP request to the container.
    - **TCP Probe**: Kubelet attempts to open a TCP connection to the container.
    - **Exec Probe**: Kubelet runs a specified command inside the container.

- **Readiness Probes**: These are used to determine if a container is ready to serve traffic. If a readiness probe fails, the endpoints controller removes the pod from the endpoints of all services that match the pod.

### 3. Container Restart Policy

- **Restart Policies**: Kubernetes allows you to specify a restart policy for containers within a pod:
  - **Always**: The container will be restarted regardless of the exit status.
  - **OnFailure**: The container will be restarted only if it exits with a non-zero status.
  - **Never**: The container will not be restarted.

### 4. Health Check Failure

- **Probe Failure**: If a liveness or readiness probe fails, the Kubelet takes appropriate action:
  - **Liveness Probe Failure**: The Kubelet kills the container. Depending on the restart policy, the container may be restarted.
  - **Readiness Probe Failure**: The Kubelet marks the pod as "NotReady", and it will not receive any traffic until the probe passes.

### 5. Pod Deletion and Recreation

- **Controller Monitoring**: Higher-level controllers (such as Deployment or ReplicaSet controllers) monitor the status of pods. If a pod fails or is deleted, these controllers ensure that a new pod is created to maintain the desired number of replicas.

### 6. Handling Node Failures

- **Node Controller**: In addition to Kubelet health checks, the Node Controller also plays a role in pod failure detection by monitoring node status. If a node becomes "NotReady", the Node Controller marks the pods on that node as failed and triggers rescheduling.

### Example Scenario: Pod Failure Detection and Recovery

1. **Kubelet on Node**: Continuously monitors the health of pods using liveness and readiness probes.
2. **Health Check Failure**:
   - Liveness probe fails for a container -> Kubelet kills the container.
   - Readiness probe fails for a container -> Kubelet marks the pod as "NotReady".
3. **Container Restart**:
   - Kubelet restarts the container based on the restart policy.
4. **Pod Failure Handling**:
   - If a pod is marked as failed and deleted, the ReplicaSet or Deployment controller detects the failure.
   - The controller creates a new pod to replace the failed one.
5. **Node Failure**:
   - Node Controller detects a node is "NotReady".
   - Marks all pods on the node as failed.
   - Higher-level controllers (ReplicaSet, Deployment) create new pods on healthy nodes.

### Summary

- **Kubelet** on each node monitors pod health using liveness and readiness probes.
- **Liveness Probes** check if a container is running, and **Readiness Probes** check if a container is ready to serve traffic.
- The **Kubelet** takes actions (e.g., restarting containers, marking pods as "NotReady") based on probe results.
- **Higher-level controllers** (e.g., ReplicaSet, Deployment) monitor and maintain the desired state of pods.
- **Node Controller** handles node-level failures and triggers rescheduling of pods.

This monitoring and recovery mechanism ensures that Kubernetes can quickly detect and recover from pod failures, maintaining the health and availability of applications running in the cluster.
