# etcd Backup and Restore in Kubernetes

etcd is a distributed key-value store that serves as the primary data store for Kubernetes, holding critical cluster data. Backing up and restoring etcd is essential for disaster recovery and ensuring the stability of the Kubernetes cluster.

## Best Practices

- Regularly schedule backups and store them in a secure, remote location.
- Test the restore process periodically to ensure that backups are valid and functional.
- Monitor etcd performance and health using Kubernetes metrics.

## Understanding etcd

### What is Stored in etcd

#### **1.Kubernetes Objects**

- **Pod Definitions**: Specifications for running containers, including metadata and desired state.
- **Service Definitions**: Configuration for services that expose applications running in pods.
- **ConfigMaps**: Configuration data that can be consumed by applications in the cluster.
- **Secrets**: Sensitive information such as passwords, OAuth tokens, and SSH keys.
- **Deployments and ReplicaSets**: Specifications for managing application deployments and scaling.
- **Namespaces**: Organizational units that provide a scope for names in Kubernetes.

#### **2.Cluster State**

- **Node Information**: Metadata about each node in the cluster, including status and resource usage.
- **Resource Quotas**: Limits on the resources available to a namespace.
- **Role-Based Access Control (RBAC) Policies**: Rules defining permissions for users and service accounts.
- **Persistent Volume Claims**: Requests for storage resources that can be fulfilled by persistent volumes.

#### **3.Scheduling Information**

- **Events**: Notifications about changes in the cluster, such as pod creations, deletions, or failures.
- **Scheduler State**: Data used by the Kubernetes scheduler to make decisions about pod placements.

#### **4.Custom Resource Definitions (CRDs)**

- Definitions for custom resources that extend Kubernetes capabilities.

---

### What is Not Stored in etcd

#### **1.Container Data**

- Actual data stored within container file systems or application data is not saved in etcd. This includes database data, logs, and temporary files.

#### **2.Static Files**

- Files on the node's filesystem or data stored in external storage systems (e.g., cloud block storage) are not kept in etcd.

#### **3.Real-time Metrics and Logs**

- Metrics related to resource usage (CPU, memory) and logs from applications or containers are not stored in etcd. These are typically handled by other systems (e.g., Prometheus, ELK stack).

#### **4.Kubernetes Controller States**

- While etcd contains the desired state of resources, it does not store the operational state or logic of controllers managing those resources.

#### **5.User Data**

- User-specific or application-specific data that is not part of Kubernetes resources is not stored in etcd.

## Backup Strategies

You can back up etcd in two primary ways:

1. **Snapshot**: A point-in-time backup of the entire etcd data store.
1. **Export**: An export of etcd key-value pairs, typically in JSON format.

## Prerequisites

### 1. Access to the Kubernetes Cluster

Ensure you have access to the Kubernetes cluster where etcd is running. This usually involves:

- A configured `kubectl` context that points to your cluster.
- Appropriate permissions to execute commands in the `kube-system` namespace (where etcd typically resides).

### 2. etcdctl Command-Line Tool

Install `etcdctl`, the command-line tool for interacting with etcd, on your local machine or a control node in your Kubernetes environment.

```bash
sudo apt-get update
sudo apt-get install etcd-client
etcdctl version
```

### 3. Access to etcd Endpoints

You need to know the endpoints of your etcd cluster, which are typically in the form of `https://<etcd-ip>:<port>`.

If etcd is configured with TLS, youâ€™ll also need:

- The certificate authority (CA) certificate file.
- The client certificate and key files for authentication.

```bash
# Example command to set the endpoints and authentication:
export ETCDCTL_API=3
export ETCDCTL_CACERT=<path-to-ca-cert>
export ETCDCTL_CERT=<path-to-client-cert>
export ETCDCTL_KEY=<path-to-client-key>

# you can find the paths to these files in ~/.minikube
```

## Backup Process

### a. Backup to a Local Location

#### Manual Backup

```bash
# 1.Identify the etcd Pod
kubectl get pods -n kube-system | grep etcd
```

```bash
# 2.Take an etcd Snapshot
kubectl exec -n kube-system <etcd-pod-name> -- etcdctl snapshot save /path/to/backup/db

# Replace `<etcd-pod-name>` with your etcd pod name and `/path/to/backup/db` with the local backup path inside the pod.
```

```bash
# 3.Copy the Snapshot to Local Machine
kubectl cp kube-system/<etcd-pod-name>:/path/to/backup/db /local/path/to/backup/db
```

#### Automated Backup Script

```yaml
# You can create a CronJob that periodically backs up etcd
# to a specified local location within the etcd pod.
apiVersion: batch/v1
kind: CronJob
metadata:
  name: etcd-backup-local
  namespace: kube-system
spec:
  schedule: "0 * * * *" # (e.g., every hour)
  jobTemplate:
    spec:
      template:
        spec:
          containers:
            - name: etcd-backup
              image: bitnami/etcd:latest
              command: ["/bin/sh", "-c"]
              args:
                - |
                  etcdctl snapshot save /backup/db
                  # Additional commands to copy the backup locally or perform cleanup
          restartPolicy: OnFailure
```

### b. Backup to a Remote Location (AWS S3)

#### Manual Backup

```bash
# 1.Identify the etcd Pod
kubectl get pods -n kube-system | grep etcd
```

```bash
# 2.Take an etcd Snapshot
kubectl exec -n kube-system <etcd-pod-name> -- etcdctl snapshot save /tmp/backup.db
```

```bash
# 3.Copy the Snapshot to AWS S3
kubectl exec -n kube-system <etcd-pod-name> -- aws s3 cp /tmp/backup.db s3://your-bucket-name/path/to/backup.db

# ensure that the AWS CLI is installed in the pod or a sidecar container that has access to the snapshot. If it's not installed, you may need to copy the snapshot to your local machine first and then upload it to S3.

# Replace `your-bucket-name` with your S3 bucket name and `/path/to/backup.db` with the desired S3 path.

```

```md
4. **Set Up IAM Permissions**
   Ensure that the IAM role associated with your Kubernetes nodes has permissions to write to the specified S3 bucket.
```

#### Automated Backup Script

```yaml
apiVersion: batch/v1
kind: CronJob
metadata:
  name: etcd-backup-s3
  namespace: kube-system
spec:
  schedule: "0 * * * *" # Adjust the schedule as needed
  jobTemplate:
    spec:
      template:
        spec:
          containers:
            - name: etcd-backup
              image: bitnami/etcd:latest
              command: ["/bin/sh", "-c"]
              args:
                - |
                  # Take snapshot
                  etcdctl snapshot save /tmp/backup.db

                  # Copy snapshot to S3
                  aws s3 cp /tmp/backup.db s3://your-bucket-name/path/to/backup/db

                  # Optional: Clean up old backups locally
          env:
            - name: AWS_ACCESS_KEY_ID
              valueFrom:
                secretKeyRef:
                  name: aws-secret
                  key: access-key
            - name: AWS_SECRET_ACCESS_KEY
              valueFrom:
                secretKeyRef:
                  name: aws-secret
                  key: secret-key
            - name: AWS_REGION
              value: your-aws-region # e.g., us-west-2
          restartPolicy: OnFailure
```

## Restore Process

### a. Restore from a Local Location

#### Manual Restore

```bash
# 1.Access the etcd Pod
kubectl exec -n kube-system <etcd-pod-name> -- sh
```

```bash
# 2.Restore the Snapshot
etcdctl snapshot restore /path/to/backup/db --data-dir /var/lib/etcd
```

```bash
# 3.Restart the etcd Pod
kubectl delete pod <etcd-pod-name> -n kube-system
```

```bash
# 4.Verify the Restore
etcdctl endpoint health
```

#### Automated Restore Script

```yaml
#!/bin/bash

# Set variables
LOCAL_SNAPSHOT="/path/to/backup.db"  # Update with the path to your local snapshot
ETCD_POD_NAME="etcd-<pod-name>"  # Replace with your actual etcd pod name

# Restore the snapshot
echo "Restoring snapshot from local location to etcd..."
kubectl exec -n kube-system $ETCD_POD_NAME -- etcdctl snapshot restore $LOCAL_SNAPSHOT --data-dir /var/lib/etcd

# Restart the etcd pod to apply changes
echo "Restarting etcd pod..."
kubectl delete pod $ETCD_POD_NAME -n kube-system

echo "Restore process completed."
```

### b. Restore from a Remote Location (AWS S3)

#### Manual Restore

```bash
# 1.Access the etcd Pod
kubectl exec -n kube-system <etcd-pod-name> -- sh
```

```bash
# 2.Copy the Snapshot from S3
# Ensure that the AWS CLI is installed and configured in the etcd pod.
aws s3 cp s3://your-bucket-name/path/to/backup.db /tmp/backup.db
```

```bash
# 3.Restore the Snapshot
etcdctl snapshot restore /tmp/backup.db --data-dir /var/lib/etcd
```

```bash
# 4.Restart the etcd Pod
kubectl delete pod <etcd-pod-name> -n kube-system
```

```bash
# 5.Verify the Restore
etcdctl endpoint health
```

#### Automated Restore Script

```yaml
#!/bin/bash

# Set variables
S3_BUCKET="your-bucket-name"  # Update with your S3 bucket name
S3_PATH="path/to/backup.db"    # Update with the S3 path to your backup
LOCAL_SNAPSHOT="/tmp/backup.db"  # Local temporary path to store the snapshot
ETCD_POD_NAME="etcd-<pod-name>"  # Replace with your actual etcd pod name

# Download the snapshot from S3
echo "Downloading snapshot from S3..."
kubectl exec -n kube-system $ETCD_POD_NAME -- aws s3 cp s3://$S3_BUCKET/$S3_PATH $LOCAL_SNAPSHOT

# Restore the snapshot
echo "Restoring snapshot to etcd..."
kubectl exec -n kube-system $ETCD_POD_NAME -- etcdctl snapshot restore $LOCAL_SNAPSHOT --data-dir /var/lib/etcd

# Restart the etcd pod to apply changes
echo "Restarting etcd pod..."
kubectl delete pod $ETCD_POD_NAME -n kube-system

echo "Restore process completed."
```
