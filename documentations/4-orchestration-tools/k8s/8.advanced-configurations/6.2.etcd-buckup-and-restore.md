# etcd Backup and Restore

**etcd Backup and Restore** is essential for maintaining the resilience and reliability of your Kubernetes (k8s) cluster. This guide outlines how to securely back up your etcd data using `etcdctl` and restore it when necessary, incorporating the use of `etcdutl` for specific deprecated commands.

## Prerequisites

Before performing backup and restore operations, ensure the following:

1. **Installation of Tools**:

   - **etcdctl**: Ensure `etcdctl` is installed and accessible in your system's PATH.
   - **etcdutl**: Ensure `etcdutl` is installed if you need to use deprecated commands.

   ```bash
   etcdctl version
   etcdutl --help
   ```

2. **Configuration**: Configure `etcdctl` and `etcdutl` with the necessary **endpoints**, **certificates**, and **keys** to securely communicate with your etcd cluster.

## Configuring `etcdctl` and `etcdutl`

Proper configuration ensures secure and accurate communication with your etcd cluster. You can configure these tools using environment variables or command-line flags.

### Using Environment Variables

Setting environment variables allows for seamless execution of multiple commands without repeatedly specifying the same parameters.

```bash
export ETCDCTL_API=3
export ETCDCTL_ENDPOINTS=https://127.0.0.1:2379 # Replace with your etcd endpoint
export ETCDCTL_CACERT=/etc/kubernetes/pki/etcd/ca.crt # Replace with etcd CA certificate path
export ETCDCTL_CERT=/etc/kubernetes/pki/etcd/server.crt # Replace with etcd server certificate path
export ETCDCTL_KEY=/etc/kubernetes/pki/etcd/server.key # Replace with etcd server key path
```

### Using Command-Line Flags

Alternatively, specify configuration parameters directly with each command using flags.

```bash
--endpoints=https://127.0.0.1:2379 \ # Replace with your etcd endpoint
--cacert=/etc/kubernetes/pki/etcd/ca.crt \ # Replace with etcd CA certificate path
--cert=/etc/kubernetes/pki/etcd/server.crt \ # Replace with etcd server certificate path
--key=/etc/kubernetes/pki/etcd/server.key # Replace with etcd server key path
```

**Note:** Ensure that certificate and key files are securely stored and have appropriate permissions to prevent unauthorized access.

## 1. **Backing Up etcd Data**

Regular backups are crucial to prevent data loss and ensure you can recover your cluster state in case of failures.

### Using `etcdctl` (etcdutl Deprecated)

**Command:**

```bash
etcdctl snapshot save <backup-file-path>
```

**Example:**

```bash
etcdctl snapshot save /backup/etcd-snapshot.db
```

**Explanation:**

- **`snapshot save`**: Initiates the backup process.
- **`/backup/etcd-snapshot.db`**: Destination path for the snapshot.
- the snapshot file format is `.db` by default.

**Backup Verification:**

After creating a snapshot, verify its integrity:

```bash
etcdutl snapshot status snapshot.db --write-out=table

```

**Expected Output:**

```ini
+----------+----------+------------+------------+
|   HASH   | REVISION | TOTAL KEYS | TOTAL SIZE |
+----------+----------+------------+------------+
| fe01cf57 |       10 |          7 | 2.1 MB     |
+----------+----------+------------+------------+
```

## 2. **Restoring etcd Data**

Restoring from a backup is essential during disaster recovery or when migrating etcd data.

### Using `etcdutl` (etcdctl Deprecated)

#### 1. **Restore the Snapshot**

```bash
etcdutl snapshot restore <snapshot-file> --data-dir=<new-data-dir>

etcdutl snapshot restore /backup/etcd-snapshot.db --data-dir=/var/lib/etcd-new
```

**Explanation:**

- **`snapshot restore`**: Initiates the restoration process.
- **`/backup/etcd-snapshot.db`**: Path to the backup snapshot.
- **`--data-dir=/var/lib/etcd-new`**: Specifies a new data directory for the restored data.

#### 2. **Replace the Old Data with Restored Data**

Now, swap out the old etcd data with the newly restored data.

```bash
# rename the old etcd data directory
sudo mv /var/lib/etcd /var/lib/etcd-backup
# rename the restored data directory to the original etcd data directory
sudo mv /var/lib/etcd-new /var/lib/etcd
```

#### 3. **Verify the Restoration**

Check the etcd cluster status to ensure the restoration was successful.

```bash
etcdctl endpoint health \
  --endpoints=https://127.0.0.1:2379 \
  --cacert=/etc/kubernetes/pki/etcd/ca.crt \
  --cert=/etc/kubernetes/pki/etcd/server.crt \
  --key=/etc/kubernetes/pki/etcd/server.key

etcdctl member list \
  --endpoints=https://127.0.0.1:2379 \
  --cacert=/etc/kubernetes/pki/etcd/ca.crt \
  --cert=/etc/kubernetes/pki/etcd/server.crt \
  --key=/etc/kubernetes/pki/etcd/server.key

```

## 3. **Best Practices for Backup and Restore**

Adhering to best practices ensures the reliability and security of your etcd backup and restoration processes.

- **Regular Backups:** Schedule automated backups to minimize data loss and ensure up-to-date snapshots.

  **Example Using Cron:**

  ```bash
  0 2 * * * /usr/local/bin/etcdctl snapshot save /backup/etcd-snapshot-$(date +\%F).db \
      --endpoints=https://127.0.0.1:2379 \
      --cacert=/etc/kubernetes/pki/etcd/ca.crt \
      --cert=/etc/kubernetes/pki/etcd/server.crt \
      --key=/etc/kubernetes/pki/etcd/server.key
  ```

- **Secure Storage:** Store backups in secure, redundant locations to protect against failures and unauthorized access. Consider using offsite storage or cloud-based solutions.

- **Test Restorations:** Periodically test restoring from backups to ensure data integrity and familiarity with the restoration process.

  **Example Test Restoration:**

  ```bash
  etcdctl snapshot restore /backup/etcd-snapshot.db \
      --data-dir=/var/lib/etcd-test
  ```

- **Monitor Backup Processes:** Implement monitoring and alerting for backup operations to detect and address issues promptly. Use tools like Prometheus or simple log monitoring scripts.

- **Version Compatibility:** Ensure that the etcd version used for restoration is compatible with the snapshot version to prevent compatibility issues.

## Backing Up and Restoring an External etcd Server in a Kubernetes Cluster

Managing the etcd cluster is crucial for the stability and reliability of your Kubernetes (K8s) environment. etcd serves as the primary data store for Kubernetes, holding all cluster state and configuration data. Regular backups and a clear restoration process are essential to safeguard against data loss and ensure quick recovery in case of failures. This guide provides a step-by-step process to back up and restore an **external etcd server** in a Kubernetes cluster.

### Prerequisites

- **Access to the etcd server and student-node:** Ensure you have SSH access to both the etcd server and the node where the snapshot is stored.
- **etcdctl Installed:** The `etcdctl` command-line tool should be installed on the etcd server.
- **Proper Certificates:** Ensure you have the necessary certificates (`ca.pem`, `etcd.pem`, `etcd-key.pem`) for secure communication with etcd.

### Backup Process Overview

1. **Create a Snapshot of etcd Data**
2. **Transfer the Snapshot to a Safe Location**
3. **Verify the Snapshot Integrity**

_Note: The following steps focus on restoring a snapshot. Ensure you have a valid snapshot before proceeding._

### Restoration Process

Follow these steps to restore an etcd snapshot to an external etcd server:

#### Step 1: Transfer the Snapshot File to the etcd Server

First, copy the etcd snapshot file from the source node (e.g., `student-node`) to the etcd server. This example copies the snapshot to the `/root` directory on the etcd server.

```bash
# On the student-node
scp /opt/cluster2.db etcd-server:/root

# Example Output:
# cluster2.db                                                                                                        100% 1108KB 178.5MB/s   00:00
```

**Explanation:**

- `scp` is used to securely copy the `cluster2.db` snapshot file from the `student-node` to the `/root` directory on `etcd-server`.
- Ensure the destination directory has sufficient space and appropriate permissions.

#### Step 2: Restore the Snapshot on the etcd Server

Execute the `etcdctl` command to restore the snapshot. Use the local endpoint (`https://127.0.0.1:2379`) and specify the new data directory where etcd will store the restored data.

```bash
# On the etcd-server
ETCDCTL_API=3 etcdctl snapshot restore /root/cluster2.db \
  --endpoints=https://127.0.0.1:2379 \
  --cacert=/etc/etcd/pki/ca.pem \
  --cert=/etc/etcd/pki/etcd.pem \
  --key=/etc/etcd/pki/etcd-key.pem \
  --data-dir /var/lib/etcd-data-new
```

**Explanation:**

- `ETCDCTL_API=3` ensures that `etcdctl` uses version 3 of the API.
- `snapshot restore` command initiates the restoration process from the specified snapshot file.
- `--data-dir /var/lib/etcd-data-new` specifies the directory where etcd will store the restored data. Choose a new directory to prevent conflicts with existing data.
- The output logs confirm successful restoration steps, including restoring the snapshot and adding cluster members.

#### Step 3: Update the etcd Systemd Service Unit File

Modify the etcd service configuration to point to the new data directory. This ensures that etcd starts with the restored data.

```bash
# On the etcd-server
vi /etc/systemd/system/etcd.service
```

**Add or Update the `--data-dir` Parameter:**

```ini
[Unit]
Description=etcd key-value store
Documentation=https://github.com/etcd-io/etcd
After=network.target

[Service]
User=etcd
Type=notify
ExecStart=/usr/local/bin/etcd \
  --name etcd-server \
  --data-dir=/var/lib/etcd-data-new \
  # (Include other necessary flags and parameters)
```

**Explanation:**

- Open the etcd service file with a text editor (`vi` in this case).
- Update the `--data-dir` flag to `/var/lib/etcd-data-new`, the directory where the snapshot was restored.
- Ensure all other necessary flags and configurations are correctly specified.

#### Step 4: Set Correct Permissions on the New Data Directory

Ensure that the etcd user owns the new data directory and has the appropriate permissions.

```bash
# On the etcd-server
chown -R etcd:etcd /var/lib/etcd-data-new

# Verify Permissions
ls -ld /var/lib/etcd-data-new/
```

**Sample Output:**

```bash
drwx------ 3 etcd etcd 4096 Jul 15 20:55 /var/lib/etcd-data-new/
```

**Explanation:**

- `chown -R etcd:etcd` recursively changes the ownership of the directory to the `etcd` user and group.
- `ls -ld` verifies that the directory permissions are correctly set to `700`, ensuring that only the `etcd` user can access it.

#### Step 5: Reload and Restart the etcd Service

Apply the changes by reloading the systemd daemon and restarting the etcd service.

```bash
# On the etcd-server
systemctl daemon-reload
systemctl restart etcd

# Check etcd Service Status
systemctl status etcd
```

**Explanation:**

- `systemctl daemon-reload` reloads the systemd manager configuration to recognize changes in the service files.
- `systemctl restart etcd` restarts the etcd service with the new configuration.
- `systemctl status etcd` helps verify that etcd is running correctly after the restart.

#### Step 6 (Optional): Restart Control Plane Components

To ensure that all control plane components (e.g., `kube-scheduler`, `kube-controller-manager`, `kubelet`) are synchronized with the restored etcd data, it's recommended to restart them. This prevents components from relying on stale data.

```bash
# On each control plane node
systemctl restart kube-scheduler
systemctl restart kube-controller-manager
systemctl restart kubelet

# Verify Status
systemctl status kube-scheduler
systemctl status kube-controller-manager
systemctl status kubelet
```

**Explanation:**

- Restarting these components ensures they reconnect to the etcd server and refresh their state based on the restored data.
- Always verify the status of each component to confirm they are running as expected.

### Best Practices

- **Regular Backups:** Schedule regular etcd snapshots to minimize data loss in case of failures.
- **Secure Storage:** Store backups in secure and redundant storage solutions.
- **Automate the Process:** Use scripts or automation tools to streamline the backup and restoration processes.
- **Test Restorations:** Periodically test the restoration process to ensure backups are valid and the procedure works as expected.

### Conclusion

Backing up and restoring an external etcd server is a critical maintenance task for Kubernetes clusters. By following the steps outlined above, you can ensure that your cluster's state is preserved and can be quickly recovered in the event of data loss or corruption. Always adhere to best practices to maintain the integrity and availability of your Kubernetes environment.

## Conclusion

Backing up and restoring etcd data using `etcdctl` and `etcdutl` is a fundamental practice for maintaining the integrity and availability of your Kubernetes clusters. By following the steps and best practices outlined above, you can ensure that your etcd data is securely backed up and can be reliably restored when necessary. This proactive approach safeguards your cluster against data loss and facilitates smooth recovery during unforeseen events.
