# Pod CIDR in Kubernetes

Understanding the Pod CIDR (Classless Inter-Domain Routing) is crucial for managing pod networking within a Kubernetes cluster. This guide covers:

1. **Who Manages Pod CIDR Allocation and How**
2. **How to Get Pod CIDR**
3. **How to Update Pod CIDR**

## 1. Who Manages Pod CIDR Allocation and How

The **Pod CIDR** is the IP address range assigned to pods within a Kubernetes cluster. Management of Pod CIDR allocation involves:

- **Kubernetes Control Plane Components**: Specifically, the **kube-controller-manager**.
- **Network Plugins (CNI Plugins)**: Such as Calico, Flannel, Weave Net, etc.

### **Pod CIDR Allocation by kube-controller-manager**

#### **ClusterCIDR and Node PodCIDRs**

- **ClusterCIDR**: The overall CIDR range for all pods in the cluster.
  - Specified using the `--cluster-cidr` flag in the `kube-controller-manager` configuration.
- **Node PodCIDR**: Subsets of the ClusterCIDR assigned to individual nodes.
  - Assigned by the kube-controller-manager if `--allocate-node-cidrs` is set to `true`.

#### **How Allocation Works**

1. **Controller Manager Configuration**:
   - **Enable CIDR Allocation**:
     - `--allocate-node-cidrs=true`
   - **Specify ClusterCIDR**:
     - `--cluster-cidr=<cluster-pod-cidr>`
2. **Node Assignment**:
   - When a node joins the cluster, the kube-controller-manager assigns a PodCIDR to it from the ClusterCIDR range.
3. **Node Network Setup**:
   - The network plugin on each node configures the network interfaces and routes based on the assigned PodCIDR.

### **Role of Network Plugins (CNI Plugins)**

- **CNI Plugins** manage the actual networking setup on each node.
- `Some plugins handle PodCIDR allocation internally and may ignore the kube-controller-manager's PodCIDR assignments` for example.
  - **Flannel**: Uses the ClusterCIDR specified in its own configuration.
  - **Calico**: Can operate in IP-in-IP mode or BGP mode, managing IP allocation through its own IPAM (IP Address Management).

### **Summary**

- **Management Responsibility**:
  - **kube-controller-manager**: Assigns PodCIDRs to nodes.
  - **CNI Plugins**: Configure networking on nodes based on assigned PodCIDRs.
- **Allocation Method**:
  - Controlled via kube-controller-manager flags.
  - Dependent on the network plugin's capabilities and configuration.

## 2. How to Get Pod CIDR

### **Method 1: Using kubectl**

#### **Retrieve PodCIDR for All Nodes**

```bash
kubectl get nodes -o jsonpath='{range .items[*]}{.metadata.name}{"\t"}{.spec.podCIDR}{"\n"}'
```

**Example Output**:

```ini
node1   10.244.0.0/24
node2   10.244.1.0/24
node3   10.244.2.0/24
```

#### **Describe a Specific Node**

```bash
kubectl describe node <node-name>
```

- Look for the `PodCIDR` field in the output.

### **Method 2: Check kube-controller-manager Configuration**

#### **Access Control Plane Node**

- SSH into the control plane node.

#### **Locate the kube-controller-manager Manifest**

- For kubeadm clusters:

  ```bash
  /etc/kubernetes/manifests/kube-controller-manager.yaml
  ```

#### **Inspect the Manifest**

```bash
cat /etc/kubernetes/manifests/kube-controller-manager.yaml | grep cluster-cidr
```

- **Example Output**:

  ```yaml
  - --cluster-cidr=10.244.0.0/16
  ```

- The value `10.244.0.0/16` is the ClusterCIDR.

### **Method 3: Network Plugin Configuration**

- Some CNI plugins have their own configuration files.

#### **Flannel Example**

- **Configuration File**:

  ```bash
  /etc/kube-flannel/net-conf.json
  ```

- **Inspect the File**:

  ```bash
  cat /etc/kube-flannel/net-conf.json | grep Network
  ```

- **Example Output**:

  ```json
  "Network": "10.244.0.0/16",
  ```

#### **Calico Example**

- **Configuration Command**:

  ```bash
  calicoctl get ippool -o yaml
  ```

- **Example Output**:

  ```yaml
  - cidr: 192.168.0.0/16
  ```

### **Method 4: Check Node Routes**

- On each node, you can check the routing table.

```bash
ip route
```

- Look for routes that correspond to PodCIDRs.

### **Note on Managed Kubernetes Services**

- In managed services like GKE, EKS, or AKS, PodCIDRs are often managed by the provider.

- Use provider-specific tools or documentation to find the PodCIDR.

## 3. How to Update Pod CIDR

Updating the Pod CIDR in a Kubernetes cluster is a complex process that can cause significant disruptions. Here's how you might approach it:

### **Approach 1: Modify the Existing Cluster**

**Warning**: This process is risky and may lead to downtime. It is recommended only for experienced administrators and should be done during maintenance windows.

#### **Steps:**

1. **Update kube-controller-manager Configuration**

   - **Edit the Manifest File**:

     ```bash
     sudo vi /etc/kubernetes/manifests/kube-controller-manager.yaml
     ```

   - **Change the `--cluster-cidr` Parameter**:

     ```yaml
     - --cluster-cidr=<new-cluster-pod-cidr>
     ```

2. **Restart kube-controller-manager**

   - Saving the manifest will restart the kube-controller-manager.

3. **Update Network Plugin Configuration**

   - **Flannel**:

     - Update `net-conf.json` with the new CIDR.

     ```bash
     sudo vi /etc/kube-flannel/net-conf.json
     ```

     - Change the `Network` field.

   - **Calico**:

     - Use `calicoctl` to delete existing IP pools and create a new one.

     ```bash
     calicoctl delete ippool <old-pool-name>
     calicoctl create -f new-ippool.yaml
     ```

4. **Restart Network Pods**

   - Delete the network plugin pods to apply the new configuration.

     ```bash
     kubectl delete pods -n kube-system -l app=flannel
     ```

5. **Update Node Configurations**

   - Each node may need to be reconfigured or restarted to recognize the new PodCIDR.

6. **Reallocate Pod IPs**

   - Existing Pods may need to be recreated to get IPs from the new CIDR.

   - Consider draining and cordoning nodes.

     ```bash
     kubectl drain <node-name> --ignore-daemonsets
     ```

### **Approach 2: Recreate the Cluster**

This is the safer and recommended approach.

#### **Steps:**

1. **Backup Current Resources**

   ```bash
   kubectl get all --all-namespaces -o yaml > all-resources.yaml
   ```

2. **Create a New Cluster with the Desired Pod CIDR**

   - **For kubeadm**:

     ```bash
     kubeadm init --pod-network-cidr=<new-cluster-pod-cidr> [other options]
     ```

   - **For Managed Services**:

     - Use provider-specific settings to specify the PodCIDR.

3. **Deploy Network Plugin**

   - Ensure the network plugin is configured with the new PodCIDR.

4. **Restore Resources**

   ```bash
   kubectl apply -f all-resources.yaml
   ```

5. **Validate the New Cluster**

   - Check that pods are running and have IPs from the new CIDR.

6. **Decommission the Old Cluster**

   - Once migration is complete, safely decommission the old cluster.

### **Important Considerations**

- **Data Persistence**: Ensure that stateful workloads have persistent storage that can be reattached.

- **DNS and Service Discovery**: Verify that DNS services are functioning correctly.

- **External Access**: Update any external services or firewall rules that depend on Pod IPs.

## **Summary**

- **Who Manages Pod CIDR Allocation**:

  - **kube-controller-manager** assigns PodCIDRs to nodes based on the `--cluster-cidr` parameter.
  - **Network Plugins (CNI)** configure the node's networking according to the assigned PodCIDR.

- **How to Get Pod CIDR**:

  - Use `kubectl` commands to retrieve PodCIDRs assigned to nodes.
  - Check kube-controller-manager and network plugin configurations.

- **How to Update Pod CIDR**:

  - **Approach 1**: Modify the existing cluster, which is complex and risky.
  - **Approach 2**: Recreate the cluster with the new Pod CIDR, which is safer and recommended.

---

**Note**: Changing the Pod CIDR is a significant operation that can impact all networking within the cluster. Always ensure you have backups and a tested recovery plan before proceeding.
