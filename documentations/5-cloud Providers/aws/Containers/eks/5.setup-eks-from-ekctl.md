# Setup EKS using eksctl

Setting up an EKS cluster using `eksctl` is straightforward and simplifies many tasks that would otherwise require manual setup. Here are the steps to set up an EKS cluster using `eksctl`:

## Prerequisites

1. **AWS Account**: Ensure you have an AWS account.
2. **IAM Permissions**: Ensure your IAM user has the necessary permissions to create and manage EKS clusters, VPCs, subnets, security groups, and EC2 instances.
3. **Install `kubectl`**: Install the Kubernetes command-line tool.
4. **Install `eksctl`**: Install `eksctl` by following the instructions below.

### Installing `eksctl`

- **Install `eksctl` on macOS using Homebrew**:

   ```sh
   brew tap weaveworks/tap
   brew install weaveworks/tap/eksctl
   ```

- **Install `eksctl` on Linux**:

   ```sh
   curl --silent --location "https://github.com/weaveworks/eksctl/releases/download/0.141.0/eksctl_$(uname -s)_amd64.tar.gz" | tar xz -C /tmp
   sudo mv /tmp/eksctl /usr/local/bin
   ```

- **Verify Installation**:

   ```sh
   eksctl version
   ```

## Step-by-Step Setup

### 1. Create an EKS Cluster

1. **Create a Cluster**:
   - Run the following command to create a cluster with default settings:

     ```sh
     eksctl create cluster --name my-cluster --region us-west-2 --nodegroup-name linux-nodes --node-type t3.medium --nodes 3 --nodes-min 1 --nodes-max 4 --managed
     ```

   - Explanation of parameters:
     - `--name`: Name of your EKS cluster.
     - `--region`: AWS region where the cluster will be created.
     - `--nodegroup-name`: Name of the node group.
     - `--node-type`: Instance type for the nodes.
     - `--nodes`: Number of nodes to create.
     - `--nodes-min`: Minimum number of nodes in the node group.
     - `--nodes-max`: Maximum number of nodes in the node group.
     - `--managed`: Specifies that the node group is managed by EKS.

2. **Wait for Cluster Creation**:
   - The cluster creation process may take several minutes. `eksctl` will create the VPC, subnets, security groups, EKS cluster, and node group.

3. **Verify Cluster**:
   - Once the cluster is created, verify it using:

     ```sh
     kubectl get nodes
     ```

### 2. Configure `kubectl`

1. **Update `kubeconfig`**:
   - Run the following command to update your `kubeconfig` file to include your new EKS cluster:

     ```sh
     aws eks --region us-west-2 update-kubeconfig --name my-cluster
     ```

2. **Verify Configuration**:
   - Ensure `kubectl` is configured correctly by listing the nodes:

     ```sh
     kubectl get nodes
     ```

### 3. Create a Fargate Profile (Optional)

If you want to use Fargate to run your pods, you need to create a Fargate profile.

1. **Create Fargate Profile**:
   - Run the following command:

     ```sh
     eksctl create fargateprofile --cluster my-cluster --name my-fargate-profile --namespace default
     ```

   - Explanation of parameters:
     - `--cluster`: Name of your EKS cluster.
     - `--name`: Name of the Fargate profile.
     - `--namespace`: Kubernetes namespace to use for Fargate pods.

### 4. Deploy Sample Application

1. **Create a Deployment**:
   - Create a deployment to run a sample application:

     ```yaml
     apiVersion: apps/v1
     kind: Deployment
     metadata:
       name: sample-deployment
     spec:
       replicas: 2
       selector:
         matchLabels:
           app: sample-app
       template:
         metadata:
           labels:
             app: sample-app
         spec:
           containers:
           - name: nginx
             image: nginx
             ports:
             - containerPort: 80
     ```

   - Save the above YAML content to a file named `deployment.yaml`.

2. **Apply Deployment**:
   - Run the following command to apply the deployment:

     ```sh
     kubectl apply -f deployment.yaml
     ```

3. **Verify Deployment**:
   - Verify that the pods are running:

     ```sh
     kubectl get pods
     ```

### 5. Set Up Cluster Autoscaler (Optional)

1. **Download Cluster Autoscaler Manifest**:
   - Download the Cluster Autoscaler deployment manifest:

     ```sh
     curl -O https://raw.githubusercontent.com/kubernetes/autoscaler/master/cluster-autoscaler/cloudprovider/aws/examples/cluster-autoscaler-one-asg.yaml
     ```

2. **Edit the Deployment Manifest**:
   - Open the downloaded file and make the following modifications:
     - **Cluster Name**: Replace `<YOUR CLUSTER NAME>` with the name of your EKS cluster.
     - **AWS Region**: Add the `--aws-region=<YOUR AWS REGION>` argument to the Cluster Autoscaler container command.
     - **Node Group**: Ensure the `nodeSelector` and `tolerations` fields match your node group configuration if applicable.

3. **Deploy the Cluster Autoscaler**:
   - Apply the modified manifest to your cluster:

     ```sh
     kubectl apply -f cluster-autoscaler-one-asg.yaml
     ```

4. **Update the Cluster Autoscaler Image**:
   - Update the Cluster Autoscaler deployment to use the latest image:

     ```sh
     kubectl set image deployment/cluster-autoscaler -n kube-system cluster-autoscaler=k8s.gcr.io/cluster-autoscaler:v1.21.0
     ```

5. **Verify the Deployment**:
   - Ensure the Cluster Autoscaler is running correctly:

     ```sh
     kubectl get pods -n kube-system -l "app.kubernetes.io/name=cluster-autoscaler"
     ```

## Additional Configurations

1. **Set Up Ingress Controller**:
   - Deploy an ingress controller (e.g., ALB Ingress Controller) to manage external access to your services:

     ```sh
     kubectl apply -f https://raw.githubusercontent.com/kubernetes-sigs/aws-alb-ingress-controller/main/docs/examples/alb-ingress-controller.yaml
     ```

2. **Configure Persistent Storage**:
   - Create Persistent Volumes (PVs) and Persistent Volume Claims (PVCs) for stateful applications as needed.
